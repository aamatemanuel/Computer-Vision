{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2524c0dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. DEEP LEARNING\n",
    "- Machine learning is turning things (data) into numbers and finding patterns. \n",
    "Traditional programming: From inputs and sets of rules, you can reproduce outputs; in a machine learning having inputs and a desired output you can find the underlying rules.  \n",
    "- Why would you use machine learning? --> For a complex problem, there could be a lot of rules. As long as you can convert a problem into numbers, you can use machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2540d-138c-4a14-b69a-4949bbc231b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Rule #1 of ML\n",
    "If you can build a simple rule-based system that doesn't require machine learning, do that. Machine learning is not a solution for everything. (GOOGLE HANDBOOK).\n",
    "- DEEP LEARNING: When the traditional approach fails, it can adapt to new scenarios, when you want to discover insights within a large collection of data. \n",
    "- BUT: Deep learning is not good if you need explainability, because the patterns learned could be uninterpretable by a human. The outputs of deep learning model aren't always predictable, they are probabilistic, and you may have errors. Don't use it when you don't have enough data. \n",
    "\n",
    "###  Machine Learning VS Deep Learning\n",
    "You want to use Machine learning with structured data (Such as spreadsheets) and Deep Learning is better for unstructured data (Image data, audio files, text).\n",
    "- For machine learning you use algorithms such as Random forest, Neares neighbour, SVM.  \n",
    "- For deep learning, you use neural networks, CNN, Transformers, RNN.  \n",
    "Depending on how you represent your problem, many algorithms can be used for both.\n",
    "\n",
    "### What are Neural Networks\n",
    "Before data (input) gets used by NN it should be turned into numbers (known as numerical encoding). Then you will feed the NN which should have been chosen an appropiate architecture according to the problem). It will learn representation (patterns/features/weights). A feature can be almost everything. You can convert the numerical outputs into a human understandable terms. \n",
    "#### Anatomy of a neural network\n",
    "Input layer, Hidden layer(s), Output layer (it learns representations or predicted probabilities).  \n",
    "Each layer is usually a combination of linear and non-linear functions. \n",
    "\n",
    "### Types of Learning\n",
    "In supervised learning, you have a lot of input data, and you also have what is the expected output of that given data, also known as labels. On the other hand, unsupervised learning the algorithm can learn patterns to figure out patterns of the similarities between different data, hence you don't have labels. Last but not least, there is Transfer Learning, in which you take the patterns learned before by a different architecture/task and take advantage of that for your own task. \n",
    "\n",
    "### Deep Learning Applications\n",
    "Recommendations in youtube (The algorithm), translation tasks, speech recognition, computer vision (object detection), Natural Language Processing, for example detecting if an email is SPAM or not SPAM. You can do classification/regresion tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af12ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. PyTorch\n",
    "It is a popular research deep learning frameworks, with Python. You can access many pre-built deep learning models, for example with torchvision. Originally it was designed to use by Facebook/Meta but now it is also used by Tesla, microsoft. 58% of the actual work done in computer vision, it is done in PyTorch (to be consulted in paperswithcode trends in 2022).\n",
    "\n",
    "What is a GPU/TPU? It is a Graphics Processing Unit which is very fast for numerical calculations. CUDA, is a parallel computing platform and application programming interface. TPU is Tensor Processing Unit. \n",
    "\n",
    "[**Tensor**](https://www.youtube.com/watch?v=f5liqUk0ZTw): It could be any representation of number, it is the fundamental block of PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a1e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2e0f0-6a19-41b9-a23b-f8bc0a0db9c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51eb94ee-4735-47c4-be65-d3307d64902d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of values and add a number to it\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9437ab0b-f21e-46bc-8dac-4712b40e6048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply it by 10\n",
    "tensor * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540db9e8-ce91-4372-af44-4f49c5c2c135",
   "metadata": {},
   "source": [
    "Notice how the tensor values above didn't end up being tensor(`[110, 120, 130]`), this is because the values inside the tensor don't change unless they're reassigned.  \n",
    "Let's subtract a number and this time we'll reassign the `tensor` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4967b83b-5461-46f7-bf16-f5cdd0690137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -8, -7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract and Reassign\n",
    "tensor = tensor - 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a66986-1b95-40f3-b802-b56865b4fe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add and reassign\n",
    "tensor = tensor + 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6aa1a-6743-4d0a-97b5-1df19ec1d73d",
   "metadata": {},
   "source": [
    "PyTorch also has a bunch of built-in functions like `torch.mul()` (short for multiplcation) and `torch.add()` to perform basic operations. For example:  \n",
    "``` python\n",
    "torch.multiply(tensor, 10)\n",
    "```\n",
    "However, it's more common to use the operator symbols like * instead of `torch.mul()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e0465f-75a3-4499-adbc-17c97b67e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n",
    "print(tensor, \"*\", tensor)\n",
    "print(\"Equals:\", tensor * tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448efd8-a864-467a-8c46-29d182cd139c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.1. Matrix multiplication \n",
    "PyTorch implements matrix multiplication functionality in the `torch.matmul()` method. _Note:_ \"@\" in Python is the symbol for matrix multiplication. The difference between **element-wise multiplication** and **matrix multiplication** is the addition of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831cac0f-9038-4f37-b6ff-e9c1bd767d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87bcda8f-802a-4680-b17b-0e81849beb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 9])\n",
      "tensor(14)\n",
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "# Element-wise matrix multiplication\n",
    "print(tensor * tensor) \n",
    "# Matrix multiplication\n",
    "print(torch.matmul(tensor, tensor))\n",
    "# Can also use the \"@\" symbol for matrix multiplication, though not recommended\n",
    "print(tensor @ tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56bea36-b20b-42f6-8ab1-fdf366c778d9",
   "metadata": {},
   "source": [
    "### 2.2. Common Errors\n",
    "Because much of deep learning is multiplying and performing operations on matrices and matrices have a strict rule about what shapes and sizes can be combined, one of the most common errors you'll run into in deep learning is shape mismatches. One example of error would be the next code snippet, because of dimensions mismatch.\n",
    "\n",
    "``` python\n",
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "# Multiplication (3x2) x (3x2)\n",
    "torch.matmul(tensor_A, tensor_B) # (this will error)\n",
    "```\n",
    "\n",
    "It is also possible to use `torch.mm()` as a shurtcut of `torch.matmul()`.  \n",
    "In this example, the dimensions can be matched, one of the ways to do this is with a **transpose** (switch the dimensions of a given tensor).\n",
    "\n",
    "You can perform transposes in PyTorch using either:\n",
    "\n",
    "- `torch.transpose(input, dim0, dim1)` - where input is the desired tensor to transpose and dim0 and dim1 are the dimensions to be swapped.\n",
    "- `tensor.T` - where tensor is the desired tensor to transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64286f66-1c9d-4611-8817-51881cec29a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "# View tensor_A and tensor_B\n",
    "print(tensor_A)\n",
    "print(tensor_B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f526c934-4df2-4224-a5f3-596be767da80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n",
      "\n",
      "New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n",
      "\n",
      "Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n",
      "\n",
      "Output:\n",
      "\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n",
      "\n",
      "Output shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# The operation works when tensor_B is transposed\n",
    "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n",
    "print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n",
    "print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n",
    "print(\"Output:\\n\")\n",
    "output = torch.matmul(tensor_A, tensor_B.T)\n",
    "print(output) \n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395677d8-b6bc-47b6-bf9b-efd02b79a529",
   "metadata": {},
   "source": [
    "The `torch.nn.Linear()` module (we'll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input `x` and a weights matrix `A`.  \n",
    "\n",
    "$y = x \\cdot A^T +b$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `x` is the input to the layer (deep learning is a stack of layers like `torch.nn.Linear()` and others on top of each other).\n",
    "- `A` is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the \"T\", that's because the weights matrix gets transposed).\n",
    "    - Note: You might also often see W or another letter like X used to showcase the weights matrix.\n",
    "- `b` is the bias term used to slightly offset the weights and inputs.\n",
    "- `y` is the output (a manipulation of the input in the hopes to discover patterns in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2dfcc56-7a83-4d33-a97b-a049e737dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n",
    "torch.manual_seed(42)\n",
    "# This uses matrix multiplication\n",
    "linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n",
    "                         out_features=6) # out_features = describes outer value \n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469949c-09c6-4122-b524-63dc90dc8c1e",
   "metadata": {},
   "source": [
    "- `in_features` relates to how many elements has each X sample. For example the tensor_A has 2 features each row.\n",
    "- `out_features` how many elements will have every row now.\n",
    "\n",
    "The linear multiplication will be done once per row in the tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc0c4e-97eb-4d61-80eb-a528067437b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3. Aggregation\n",
    "There are some ways that allow to manipulate tensors, and aggregate (go from more values to less values) is one of this ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3572bbae-4467-4c7f-913d-ee4ef0dfefa1",
   "metadata": {},
   "source": [
    "#### 2.3.1. Max, Min, Mean, Sum\n",
    "First we'll create a tensor and then find the max, min, mean and sum of it. \n",
    "\n",
    "Note: You may find some methods such as `torch.mean()` require tensors to be in `torch.float32` (the most common) or another specific datatype, otherwise the operation will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53e25f6e-384d-4c8e-a359-50b23cae7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "Minimum: 0\n",
      "Maximum: 90\n",
      "Mean: 45.0\n",
      "Sum: 450\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.arange(0, 100, 10)\n",
    "print(x) \n",
    "\n",
    "# AGGREGATION\n",
    "print(f\"Minimum: {x.min()}\")\n",
    "print(f\"Maximum: {x.max()}\")\n",
    "# print(f\"Mean: {x.mean()}\") # this will error\n",
    "print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\n",
    "print(f\"Sum: {x.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63a008-ba34-457d-b485-a0820c4a4ecb",
   "metadata": {},
   "source": [
    "You can also do the same as above with torch methods.\n",
    "```python\n",
    "torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32ecba-3605-43d2-8abf-39abf4c07435",
   "metadata": {},
   "source": [
    "#### 2.3.2. Positional min/max\n",
    "You can also find the index of a tensor where the max or minimum occurs with `torch.argmax()` and `torch.argmin()` respectively.\n",
    "\n",
    "This is helpful incase you just want the position where the highest (or lowest) value is and not the actual value itself (we'll see this in a later section when using the softmax activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80634d9a-b45e-4f65-b1ed-c2b44743e57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "Index where max value occurs: 8\n",
      "Index where min value occurs: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "tensor = torch.arange(10, 100, 10)\n",
    "print(f\"Tensor: {tensor}\")\n",
    "\n",
    "# Returns index of max and min values\n",
    "print(f\"Index where max value occurs: {tensor.argmax()}\")\n",
    "print(f\"Index where min value occurs: {tensor.argmin()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9d36b-b74a-4330-9fed-6297974f3c87",
   "metadata": {},
   "source": [
    "### 2.4. Tensor datatype\n",
    "A common issue with deep learning operations is having your tensors in different datatypes. If one tensor is in `torch.float64` and another is in `torch.float32`, you might run into some errors.  \n",
    "You can change the datatypes of tensors using `torch.Tensor.type(dtype=None)` where the dtype parameter is the datatype you'd like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b7c30e1-6c45-4ef7-b4f6-3cd38e6fc357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor and check its datatype\n",
    "tensor = torch.arange(10., 100., 10.)\n",
    "print(tensor.dtype)\n",
    "\n",
    "# Create a float16 tensor\n",
    "tensor_float16 = tensor.type(torch.float16)\n",
    "print(tensor_float16) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30294ea2-52a5-4c7c-8e2d-4f3b8688e792",
   "metadata": {},
   "source": [
    "### 2.5. Reshaping\n",
    "Often times you'll want to reshape or change the dimensions of your tensors without actually changing the values inside them.  \n",
    "To do so, some popular methods are: reshape, view, stack, squeeze, unsqueeze, permute.\n",
    "\n",
    "Why using them? Because deep learning models (neural networks) are all about manipulating tensors in some way. And because of the rules of matrix multiplication, if you've got shape mismatches, you'll run into errors. These methods help you make the right elements of your tensors are mixing with the right elements of other tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "195e093c-c228-4b78-9a7e-feed57c23019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5., 6., 7.]) torch.Size([7])\n",
      "tensor([[1., 2., 3., 4., 5., 6., 7.]]) torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1., 8.)\n",
    "print(x, x.shape)\n",
    "\n",
    "# ADDING AN EXTRA DIMENSION\n",
    "x_reshaped = x.reshape(1, 7)\n",
    "print(x_reshaped, x_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae5079a8-cbe6-45f3-8b40-60f4d57a4646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change view (keeps same data as original but changes view)\n",
    "# See more: \n",
    "z = x.view(1, 7)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80079267-dade-4aee-82ed-8c4c5ec74358",
   "metadata": {},
   "source": [
    "Changing the [view](https://stackoverflow.com/a/54507446/7900723) of a tensor with `torch.view()` really only creates a new view of the same tensor.  \n",
    "So changing the view changes the original tensor too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cd6afe9-5f92-4f2b-b4d3-fc85ccf6bfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing z changes x\n",
    "z[:, 0] = 5\n",
    "z, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a3a85e2-cae5-4096-8191-90f9a0ba1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 2., 3., 4., 5., 6., 7.],\n",
      "        [5., 2., 3., 4., 5., 6., 7.],\n",
      "        [5., 2., 3., 4., 5., 6., 7.],\n",
      "        [5., 2., 3., 4., 5., 6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "# STACK tensors on top of each other\n",
    "x_stacked = torch.stack([x, x, x, x], dim=0) # Try changing dim to dim=1 and see what happens\n",
    "# DIM 0 --> You add the vector as a row\n",
    "# DIM 1 --> You add the vector as a column\n",
    "print(x_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f22f6-208f-4ed3-8530-3f6244248b80",
   "metadata": {},
   "source": [
    "Removing all single dimensions from a tensor: To do so you can use `torch.squeeze()` (I remember this as squeezing the tensor to only have dimensions over 1). It is possible to do the reverse effect with `torch.unsqueeze()` to add a dimension value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac7a3d79-3a32-4020-b80f-32ad4a351c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "Previous shape: torch.Size([1, 7])\n",
      "\n",
      "New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "New shape: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_reshaped}\")\n",
    "print(f\"Previous shape: {x_reshaped.shape}\")\n",
    "\n",
    "# Remove extra dimension from x_reshaped\n",
    "x_squeezed = x_reshaped.squeeze()\n",
    "print(f\"\\nNew tensor: {x_squeezed}\")\n",
    "print(f\"New shape: {x_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56bd05b-b73c-4f98-9b9e-04236ac39147",
   "metadata": {},
   "source": [
    "**Permute:** can also rearrange the order of axes values with `torch.permute(input, dims)`, where the input gets turned into a view with new dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f775a1ca-32d2-44a2-b076-0f7bdf47697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 224, 3])\n",
      "New shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Create tensor with specific shape\n",
    "x_original = torch.rand(size=(224, 224, 3))\n",
    "\n",
    "# Permute the original tensor to rearrange the axis order\n",
    "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c96d88-212f-4746-8b02-3f8348927960",
   "metadata": {},
   "source": [
    "### 2.6. Indexing\n",
    "Sometimes you want to select specific data from tensors (for example, only the first column or second row). To do so, you can use indexing, which is really similar to NumPy arrays. In order to make a more general example, we create a tensor with depth = 3, so  multiple parts of indexing can be observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fae40e78-52c4-4a57-aab3-b63be7c3e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24],\n",
      "         [25, 26, 27]]]) torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 28).reshape(3, 3, 3)\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abb3c86b-84fb-4a5e-9ba1-5deb1be298bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First square bracket:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Second square bracket: tensor([1, 2, 3])\n",
      "Third square bracket: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's index bracket by bracket\n",
    "# It takes the channel, or the depth of it; x[1] would print the second submatrix\n",
    "print(f\"First square bracket:\\n{x[0]}\") \n",
    "print(f\"Second square bracket: {x[0][0]}\") \n",
    "print(f\"Third square bracket: {x[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8e5234b-e3ed-47b2-a976-ab44804a6fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [10, 11, 12],\n",
      "        [19, 20, 21]])\n"
     ]
    }
   ],
   "source": [
    "# Get all values of 0th dimension and the 0 index of 1st dimension\n",
    "# BECAUSE THERE ARE 3 CHANNELS, IT IS TAKING THE 1st row of the 3 channels\n",
    "print(x[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa435c03-157c-4679-8c03-8a26293a0983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  5,  8],\n",
      "        [11, 14, 17],\n",
      "        [20, 23, 26]])\n"
     ]
    }
   ],
   "source": [
    "# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n",
    "print(x[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77501478-5d3c-40cc-81d5-789569ba6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5, 14, 23])\n"
     ]
    }
   ],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "print(x[:, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f727d7-bd2e-46d6-8b3e-416c7a4207a2",
   "metadata": {},
   "source": [
    "For simplicity, we do the same example but now with just depth = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3392d44-51db-4571-89fd-27d1ed8b633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]]) torch.Size([1, 3, 3])\n",
      "tensor([[1, 2, 3]])\n",
      "tensor([[2, 5, 8]])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "print(x, x.shape)\n",
    "\n",
    "# Get all values of 0th dimension and the 0 index of 1st dimension\n",
    "# IT IS TAKING THE 1st row of all the channel\n",
    "print(x[:, 0])\n",
    "\n",
    "# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n",
    "print(x[:, :, 1])\n",
    "\n",
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "print(x[:, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912007cc-2ab3-48bf-b2da-d18e7fa3aca1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.7. Numpy / PyTorch\n",
    "NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely. The two main methods you'll want to use for NumPy to PyTorch (and back again) are:\n",
    "\n",
    "- `torch.from_numpy(ndarray)` - NumPy array -> PyTorch tensor.\n",
    "- `torch.Tensor.numpy()` - PyTorch tensor -> NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b0743ca-9107-46ae-b236-9e7ffcd3d9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ccf166-bdf3-4d27-9be9-7b1cb007b6b0",
   "metadata": {},
   "source": [
    "By default, NumPy arrays are created with the datatype `float64` and if you convert it to a PyTorch tensor, it'll keep the same datatype (as above). However, many PyTorch calculations default to using `float32`.\n",
    "\n",
    "If you want to convert your NumPy array (float64) -> PyTorch tensor (float64) -> PyTorch tensor (float32), you can use:\n",
    "\n",
    "- `tensor = torch.from_numpy(array).type(torch.float32)`.\n",
    "\n",
    "It will always keep the same data type when you do the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f0b63f1-1a27-4bc9-adc2-2cc8f6fbf7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]), tensor([1., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array).type(torch.float32)\n",
    "print(tensor.dtype)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c8ac50c-c1eb-40c0-b0a7-60c062861ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor to NumPy array\n",
    "tensor = torch.ones(7) # create a tensor of ones with dtype=float32\n",
    "numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c963b69-f824-49e8-b0bb-db28cb1f5b67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.8. Reproducibility\n",
    "In neural networks and machine learning, randomness plays a big role.  \n",
    "Well, pseudorandomness that is. Because after all, as they're designed, a computer is fundamentally deterministic (each step is predictable) so the randomness they create are simulated randomness (though there is debate on this too, but since I'm not a computer scientist, I'll let you find out more yourself).\n",
    "\n",
    "How does this relate to neural networks and deep learning then?\n",
    "We've discussed neural networks start with random numbers to describe patterns in data (these numbers are poor descriptions) and try to improve those random numbers using tensor operations (and a few other things we haven't discussed yet) to better describe patterns in data.\n",
    "\n",
    "In short:\n",
    "`start with random numbers -> tensor operations -> try to make better` (Iterative)\n",
    "\n",
    "Although randomness is nice and powerful, sometimes you'd like there to be a little less randomness.\n",
    "Why? So you can perform repeatable experiments. For example, you create an algorithm capable of achieving X performance. And then your friend tries it out to verify you're not crazy.\n",
    "\n",
    "How could they do such a thing? That's where **reproducibility** comes in. In other words, can you get the same (or very similar) results on your computer running the same code as I get on mine?\n",
    "\n",
    "Let's see a brief example of reproducibility in PyTorch. We'll start by creating two random tensors, since they're random, you'd expect them to be different right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "08c7d25f-8847-4afe-9e33-70a4abc2e38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.8016, 0.3649, 0.6286, 0.9663],\n",
      "        [0.7687, 0.4566, 0.5745, 0.9200],\n",
      "        [0.3230, 0.8613, 0.0919, 0.3102]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[0.9536, 0.6002, 0.0351, 0.6826],\n",
      "        [0.3743, 0.5220, 0.1336, 0.9666],\n",
      "        [0.9754, 0.8474, 0.8988, 0.1105]])\n",
      "\n",
      "Does Tensor A equal Tensor B? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create two random tensors\n",
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1160c1-03fa-4a41-9fe2-7a8781804da4",
   "metadata": {},
   "source": [
    "The tensors come out with different values. But what if you wanted to created two random tensors with the same values. As in, the tensors would still contain random values but they would be of the same flavour.\n",
    "\n",
    "That's where `torch.manual_seed(seed)` comes in, where seed is an integer (like 42 but it could be anything) that flavours the randomness. Let's try it out by creating some more flavoured random tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d4529d9-283b-4d74-96bf-01e3c9e6867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor D:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Does Tensor C equal Tensor D? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set the random seed\n",
    "RANDOM_SEED = 42 # try changing this to different values and see what happens to the numbers below\n",
    "torch.manual_seed(seed=RANDOM_SEED) \n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "# Have to reset the seed every time a new rand() is called \n",
    "# Without this, tensor_D would be different to tensor_C \n",
    "\n",
    "# THE NEXT LINE SETS PYTHON SEED EQUAL\n",
    "torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "random_tensor_C == random_tensor_D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
