{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad99da5-be85-4458-82c8-9f74e2273376",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Computer Vision Traditional Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2835b-361d-4cf0-8516-0b7afb3d3228",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Course: Computer Vision - Master of Artificial Intelligence\n",
    "Understand traditional methods for computer vision, which at the same time had a technical goal on how to use OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a27fd5-6baa-48db-bceb-3c00965ebf52",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./Project1.gif\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6897fef-bc1c-44ac-9043-562bbd0c9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "##############################################################################\n",
    "INITIALIZE VIDEO PARAMETERS\n",
    "##############################################################################\n",
    "'''\n",
    "\n",
    "# PROJECT: ASSIGNMENT 1 - FROM BASIC IMAGE PROCESSING TOWARDS OBJECT DETECTION\n",
    "# HECTOR MANUEL ARTEAGA AMATE --> r0819325\n",
    "\n",
    "source = 'Video_CV.mp4'\n",
    "# VIDEO CAPTURE OBJECT\n",
    "video_cap = cv2.VideoCapture(source)\n",
    "height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# RESIZE THE OUTPUT FRAME\n",
    "w_out = int(width*0.8)\n",
    "h_out = int(height*0.8)\n",
    "\n",
    "# CONFIG FOR OUTPUT VIDEO\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('Assignment1_Arteaga_Hector.mp4', fourcc, fps, (w_out, h_out))\n",
    "win_name = 'Video Preview'\n",
    "cv2.namedWindow(win_name)\n",
    "\n",
    "# IMAGE TO BE USED IN TEMPLATE MATCHING\n",
    "# TEMPLATE MATCHING IS MAINLY USED IN GRAY SCALE\n",
    "template = cv2.imread('Wax_template.png', 0)\n",
    "h, w = template.shape\n",
    "\n",
    "# IMAGE TO BE USED IN CARTE BLANCHE\n",
    "ball = cv2.imread('Volleyball.png', cv2.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f844195-2039-4cdc-849f-07d1b22828ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##############################################################################\n",
    "DEFINITION OF FUNCTIONS\n",
    "1. Print text in certain frame.\n",
    "2. Gaussian Blur\n",
    "3. Bilateral Filter\n",
    "4. Morphological Transformations: Dilation and Erosion.\n",
    "5. Sobel Operator \n",
    "##############################################################################\n",
    "'''\n",
    "\n",
    "# SUBTITLES\n",
    "def subtitle(frame, text):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    img_sub = cv2.putText(frame, text, (340, 40), font, 1, (255, 0, 0), 3)\n",
    "    return img_sub\n",
    "\n",
    "# FILTERING\n",
    "def gaussian_blur(frame, kernel, sigmaX):\n",
    "    gblur = cv2.GaussianBlur(frame, kernel, sigmaX)\n",
    "    return gblur\n",
    "\n",
    "def bilateral_blur(frame, sigma, filter_size):\n",
    "    sigma_color = sigma\n",
    "    sigma_space = sigma\n",
    "    bblur = cv2.bilateralFilter(frame, filter_size, sigma_color, sigma_space)\n",
    "    return bblur\n",
    "\n",
    "# DILATION - MORPHOLOGICAL TRANSFORMATIONS\n",
    "def dilation(img, selector):\n",
    "    # Convert to HSV SCALE\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    # CREATE A BLUE IMAGE TO BE USED TO SHOW THE EFFECT OF THE\n",
    "    \n",
    "    # MORPHOLOGICAL TRANSFORMATION\n",
    "    bkg = np.zeros((img.shape[0], img.shape[1], 3))\n",
    "    bkg[:] = 255, 0, 0\n",
    "    # Lower and Upper boundaries for detection of colors in HSV format\n",
    "    # Color = RED\n",
    "    l1 = np.array([170, 130, 185])\n",
    "    u1 = np.array([180, 255, 255])\n",
    "    # Create a mask for defined color\n",
    "    mask1 = cv2.inRange(hsv, l1, u1)\n",
    "    # APPLY MORPHOLOGICAL TRANSFORMATION (MT)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dil = cv2.dilate(mask1, kernel, iterations=1)\n",
    "    # CALCULATE THE EFFECT OF THE MT\n",
    "    diff = dil - mask1\n",
    "\n",
    "    if selector == 0:\n",
    "        img_det = cv2.cvtColor(mask1, cv2.COLOR_GRAY2BGR)\n",
    "    if selector == 1:\n",
    "        img_det = cv2.bitwise_and(img, img, mask=dil)  # mask1\n",
    "    if selector == 2:\n",
    "        # SHOW MT IN DIFFERENT COLOR\n",
    "        # Calculate the dilation effect and show it in blue color\n",
    "        mt_eff = cv2.bitwise_and(bkg, bkg, mask=diff)\n",
    "        img_det = cv2.bitwise_and(img, img, mask=mask1)\n",
    "        dil_2 = img_det + mt_eff\n",
    "        img_det = dil_2.astype(np.uint8)\n",
    "    return img_det\n",
    "\n",
    "def sobel_operator(img, dx, dy):\n",
    "    bkg = np.zeros((img.shape[0], img.shape[1], 3))\n",
    "    bkg[:] = 0, 255, 0\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    sobel = cv2.Sobel(img2, cv2.CV_64F, dx, dy, ksize=5)\n",
    "    cv2.normalize(sobel, dst=sobel, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_64F)\n",
    "    # SOBEL IS COMPUTED AS A FLOAT NUMBER\n",
    "    \n",
    "    # First is normalized, then converted to integer between 0 and 255\n",
    "    sobel_int = (255 * sobel).astype(np.uint8)\n",
    "\n",
    "    # VISUALIZE IN COLOR\n",
    "    # The algorithm used was similar to the one used to show\n",
    "    # the effect of the morphological transformation. The edges\n",
    "    # detected by sobel are mainly black or white, so a mask is\n",
    "    # created and then show this mask in green.\n",
    "    l1 = np.array([160, 160, 160])\n",
    "    u1 = np.array([255, 255, 255])\n",
    "    l2 = np.array([0, 0, 0])\n",
    "    u2 = np.array([90, 90, 90])\n",
    "    mask1 = cv2.inRange(sobel_int, l1, u1)\n",
    "    mask2 = cv2.inRange(sobel_int, l2, u2)\n",
    "    mask3 = mask1 + mask2\n",
    "    res = cv2.bitwise_and(bkg, bkg, mask=mask3)\n",
    "    res = res.astype(np.uint8)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4cc7b-8d70-4a58-b97f-565af5d9741c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 2. Applied Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047ad9e-4678-4735-ba5a-6d028fe44e42",
   "metadata": {},
   "source": [
    "#### Research project: Increase autonomy of AMR through vision\n",
    "There is AMR for navigation through complex warehouses, with varying dynamic constraints. Now the intention is to add visual perception to the vehicle to detect parking slots and then to automatically perform a parking maneuver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e1cce-b412-4ae6-959e-4bcc6a17d226",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./AMR.png\" alt=\"drawing\" width=\"500\"/>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104eba8-7289-49fb-9334-1581e43ba3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectLines(picture, parameters):\n",
    "    \"\"\"\n",
    "    FUNCTION TO DETECT LINES FROM THE PICTURE\n",
    "    - Filtering to reduce noise, but retain edges\n",
    "    - Sobel edge detector || Canny edge detector\n",
    "    - Hough transform\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\"########## FILTERING ##########\"\"\"\n",
    "    ## Blurring:\n",
    "    # blurred_image = cv2.blur(left_camera_flip,(3,3),0,)\n",
    "\n",
    "    ## Gaussian blurring\n",
    "    gaussian_blurred_image = cv2.GaussianBlur(picture, (3, 3), 0)\n",
    "\n",
    "    ## Bilateral filtering\n",
    "    # bilateral_filter_image = cv2.bilateralFilter(picture, 9, 10, 0, cv2.BORDER_ISOLATED)\n",
    "\n",
    "    \"\"\"\"########## EDGE ENHANCEMENT ##########\"\"\"\n",
    "\n",
    "    # # SOBEL OPERATOR\n",
    "    # bilateral_filter_image_normalized = bilateral_filter_image / 255. # the image has to be normalized\n",
    "    # sobel_x = cv2.Sobel(bilateral_filter_image_normalized, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    # sobel_y = cv2.Sobel(bilateral_filter_image_normalized, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    # cv2.normalize(sobel_x, dst=sobel_x, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_64F)\n",
    "    # cv2.normalize(sobel_y, dst=sobel_y, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_64F)\n",
    "    # sobel = np.sqrt(sobel_x ** 2 + sobel_y ** 2)\n",
    "    # sobel_enhanced = cv2.sqrt(cv2.addWeighted(cv2.pow(sobel_x, 2.0), 1.0, cv2.pow(sobel_y, 2.0), 1.0, 0.0))\n",
    "\n",
    "    ## CANNY EDGE DETECTOR\n",
    "    canny = cv2.Canny(gaussian_blurred_image, parameters.cannyTresh.value, 255)\n",
    "\n",
    "    \"\"\"\"########## THRESHOLDING ##########\"\"\"\n",
    "    ## THRESHOLD\n",
    "    thresh = parameters.edgeTreshold.value\n",
    "    maxValue = 255\n",
    "    retval, thresholded = cv2.threshold(gaussian_blurred_image, thresh, maxValue, cv2.THRESH_BINARY)\n",
    "    # retval, sobel_threshold = cv2.threshold(sobel_enhanced, thresh, maxValue, cv2.THRESH_BINARY)\n",
    "    if parameters.thresholdOrCanny.value:\n",
    "        useMe = thresholded\n",
    "    else:\n",
    "        useMe = canny\n",
    "    cv2.imshow('Threshold or Canny', useMe)\n",
    "\n",
    "    \"\"\"\"########## MORPHOLOGICAL TRANSFORMATIONS ##########\"\"\"\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, \n",
    "                                       (parameters.kernelmorph.value, parameters.kernelmorph.value))\n",
    "    if parameters.morphIterations.value != 0:\n",
    "        if parameters.dilationOrErosion.value == 0:\n",
    "            morph = cv2.dilate(useMe, kernel, iterations=parameters.morphIterations.value)\n",
    "        if parameters.dilationOrErosion.value == 1:\n",
    "            morph = cv2.erode(useMe, kernel, iterations=parameters.morphIterations.value)\n",
    "\n",
    "    else:\n",
    "        morph = useMe\n",
    "    cv2.imshow('Morphological', morph)\n",
    "\n",
    "    \"\"\"\"########## HOUGH TRANSFORM ##########\"\"\"\n",
    "\n",
    "    dilate_color = cv2.cvtColor(morph, cv2.COLOR_GRAY2RGB)\n",
    "    hough_lines = cv2.HoughLinesP(morph,\n",
    "                                  rho = 1,\n",
    "                                  theta = np.pi / 180,\n",
    "                                  threshold = parameters.houghthresh.value,\n",
    "                                  minLineLength=parameters.minLineLength.value,\n",
    "                                  maxLineGap=parameters.maxLineGap.value)\n",
    "\n",
    "    count = 0\n",
    "    if hough_lines is not None:\n",
    "        for line in hough_lines:\n",
    "\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(dilate_color, (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "            if count == parameters.maxNbLines.value:\n",
    "                ## PIECE OF CODE TO ANALYZE FRAME BY FRAME\n",
    "                # list_of_lines = np.asarray(hough_lines)\n",
    "                # np.save('list_of_lines.py', list_of_lines)\n",
    "                # cv2.imwrite(\"Frame.png\", dilate_color_without_lines)\n",
    "                # exit()\n",
    "                break\n",
    "            count += 1\n",
    "\n",
    "    return dilate_color, hough_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45b364-585f-41bd-a8bc-1b44029e3705",
   "metadata": {
    "tags": []
   },
   "source": [
    "&nbsp;\n",
    "<p align=\"center\">\n",
    "<img src=\"./Line_detection.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e876773-264d-42a1-a5fe-7fb85d0ba972",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Deep Learning WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790df7d3-37e1-4cb2-957e-b0d8036018af",
   "metadata": {},
   "source": [
    "#### 3.1. Loading data (Datasets)\n",
    "\n",
    "Datasets: VGG Face dataset, Pascal VOC 2009, nuScenes\n",
    "&nbsp;\n",
    "<p align=\"center\">\n",
    "<img src=\"./Face.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2cbb6-bfd2-41c5-ace6-5784059421c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train_set.csv', index_col = 0)\n",
    "train.index = train.index.rename('id')\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test_set.csv', index_col = 0)\n",
    "test.index = test.index.rename('id')\n",
    "\n",
    "# read the images as numpy arrays and store in \"img\" column\n",
    "train['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in train.iterrows()]\n",
    "\n",
    "test['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in test.iterrows()]\n",
    "  \n",
    "\n",
    "train_size, test_size = len(train),len(test)\n",
    "\n",
    "\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3a250-f810-4b42-820f-b9ad9fce4199",
   "metadata": {},
   "source": [
    "### 3.2. Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d06b1a-19cc-4c7f-ad7c-a131e3135e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range = 45, \n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1, \n",
    "                         shear_range = 0, \n",
    "                         zoom_range = 0.1, \n",
    "                         channel_shift_range = 30., \n",
    "                         horizontal_flip=True)\n",
    "\n",
    "# batch_size --> (How many Images is generating)\n",
    "batch_size = 5\n",
    "train_X_aug = []\n",
    "train_y2 =  np.empty((batch_size*train_X.shape[0],), int)\n",
    "\n",
    "for i in range(len(train_X)):\n",
    "    x = train_X[i]\n",
    "    # INPUT SIZE\n",
    "    # IMAGES, SIZE[0], SIZE[1], # Channels\n",
    "    x = x.reshape((1, ) + x.shape)\n",
    "    aug_iter = gen.flow(x, batch_size=batch_size)\n",
    "    aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(batch_size)]\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        train_X_aug.append(aug_images[j])\n",
    "        train_y2[(i*5)+j] = train_y[i]\n",
    "        \n",
    "train_X_aug = np.array(train_X_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf34d1-c642-4537-92a4-057ae3bad57a",
   "metadata": {},
   "source": [
    "### 3.3. Feature Representations\n",
    "PCA --> Feature Extractor for dimensionality reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a5cfa-8905-4a26-8d3e-d4b049b3e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \n",
    "    def __init__(self, n_components, face_size, whiten=False):\n",
    "        self.n_components = n_components\n",
    "        self.FACE_SIZE = face_size\n",
    "        self.whiten = whiten\n",
    "    \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.mean = np.mean(X, axis=0, dtype=np.float64)\n",
    "        self.mean_face = self.mean.reshape(self.FACE_SIZE)\n",
    "\n",
    "        centered_data = X - self.mean\n",
    "        # ECONCOMIC SVD\n",
    "        U, S, Vt = np.linalg.svd(centered_data, full_matrices=False)\n",
    "        \n",
    "        # CHANGING SIGNS\n",
    "        max_abs_cols = np.argmax(np.abs(U), axis=0)\n",
    "        signs = np.sign(U[max_abs_cols, range(U.shape[1])])\n",
    "        U *= signs\n",
    "        Vt *= signs[:, np.newaxis]\n",
    "\n",
    "        components = Vt\n",
    "\n",
    "        self.U_vector = U[0:self.n_components]\n",
    "        self.S_vector = S[0:self.n_components]\n",
    "\n",
    "        self.components = components[0:self.n_components]\n",
    "        self.singular_values = singular_values[:self.n_components]\n",
    "\n",
    "        eigenFaces = [];\n",
    "        for i in range(0, self.n_components):\n",
    "            eigenFace = np.reshape(self.components[i,:], self.FACE_SIZE)\n",
    "            eigenFaces.append(eigenFace)\n",
    "        self.eigenFaces = eigenFaces\n",
    "        return U, S, Vt\n",
    "    \n",
    "    def transform(self, X):\n",
    "        U, S, Vt = self.fit(X)\n",
    "        U = U[:, :self.n_components] * S[:self.n_components]\n",
    "        # X_proj = X*V = U*S*Vt*V = U*S\n",
    "        return U\n",
    "        \n",
    "    def inverse_transform(self, X):\n",
    "        return np.dot(X, self.components) + self.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7a593-571a-49e6-8860-27a7500f8d04",
   "metadata": {},
   "source": [
    "### 3.4. Classifier\n",
    "MLP, Convolutional Neural Networks, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611fd12-4c76-4edf-8211-ad113ce6d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork:\n",
    "    def fit(self, X, y):\n",
    "        self.cnn_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=36, kernel_size=(3,3), activation=tf.nn.relu),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=36, kernel_size=(3,3), activation=tf.nn.relu),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "            \n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "        ])\n",
    "        \n",
    "        #MODEL PARAMETERS\n",
    "        self.cnn_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=2),\n",
    "                               loss='categorical_crossentropy',\n",
    "                               metrics=['accuracy'])\n",
    "        self.Y_train = tf.keras.utils.to_categorical(y)\n",
    "        \n",
    "        #TRAIN\n",
    "        self.cnn_model.fit(X, self.Y_train, epochs=20, batch_size=32)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predict_y = self.model.predict(X) \n",
    "        return np.argmax(predict_y,axis=1)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cd9f6-71fb-43c5-b15f-4c96951071e9",
   "metadata": {},
   "source": [
    "### 3.5. CNN Architectures \n",
    "Transfer Learning, Fine Tunning.  \n",
    "Note: This model was built to train for Pascal VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1291e2-623c-4a76-baac-632d91efe8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_inception():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    # base_model.summary()\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    predictions = layers.Dense(20, activation = 'sigmoid')(x)\n",
    "    head_model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    return head_model\n",
    "\n",
    "head_model = classifier_inception()\n",
    "head_model.compile(optimizer= optimizers.Adam(), loss=losses.binary_crossentropy, metrics=['accuracy'])\n",
    "# head_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0f2de-1433-47b7-858c-e0d718d750fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 4. Thesis Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f8462-7f91-44bd-8fee-d64b1bf6f01f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./Synthetic_Real.png\" alt=\"drawing\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "Visual patches based on GPS information. \n",
    "<p align=\"center\">\n",
    "<img src=\"./scene-0048.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29230a-e9c5-4348-b771-318bc6c13831",
   "metadata": {},
   "source": [
    "### 4.1. PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008f5a7-7c02-45b1-b1c4-fde2fee4a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=3):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "in_channels = 3\n",
    "num_classes = 3\n",
    "learning_rate = 3e-4 # karpathy's constant\n",
    "batch_size = BATCH_SIZE\n",
    "num_epochs = 250\n",
    "\n",
    "# Initialize network\n",
    "model = CNN(in_channels=in_channels, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15c48d-79fe-45cb-b664-1367a4070990",
   "metadata": {},
   "source": [
    "### 4.2. Post-processing\n",
    "Metrics: Accuracy, Confusion Matrix, Triplet loss, Dice Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d85d153-bdbf-49ed-b631-7731af0b0fab",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./Accuracy.png\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
